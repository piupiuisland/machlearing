{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path ='/Users/jiayun/PycharmProjects/untitled/untitled/untitled/r' \\\n",
    "      'bot Project/ my code/machine learning/week3 logistic regression/ex2data1.txt'\n",
    "\n",
    "data = pd.read_csv(path, header = None, names = [\"Exam 1\", \"Exam 2\", \"Admitted\"])\n",
    "data.head()\n",
    "\n",
    "'''1'''\n",
    "##############################################################################\n",
    "########---------------------plot 2 class part--------------------------########\n",
    "##############################################################################\n",
    "positive = data[data['Admitted'].isin([1])]\n",
    "negative = data[data['Admitted'].isin([0])]\n",
    "\n",
    "'''data['Admitted'].isin([1])   返回了一个false true 的矩阵， 外面再加一个data【】， 就实现了在这个dataframe里面提取出找到的'1'\n",
    "另外一种写法：\n",
    "## positive = data[data.iloc[:,2].isin([1])] ##\n",
    " 这个和上面的命令一模一样， 只不过换了一个说法， data.iloc[:,1]\n",
    "'''\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (12,8))\n",
    "# ax.scatter(positive['Exam 1'], positive['Exam 2'], s = 50 ,c = 'b', marker = 'o', label = 'Admitted')\n",
    "# ax.scatter(negative['Exam 1'], negative['Exam 2'], s = 50, c = 'r', marker = 'x', label = 'Rejected')\n",
    "# ax.legend( loc = 1 )   # 说明小图的位置， 默认是1，1 是右上角\n",
    "# ax.set_xlabel('Exam 1 score')\n",
    "# ax.set_ylabel('Exam 2 score')\n",
    "# plt.show()\n",
    "\n",
    "##############################################################################\n",
    "########---------------------plot 2 class end--------------------------########\n",
    "##############################################################################\n",
    "\n",
    "'''2'''\n",
    "##############################################################################\n",
    "########---------------------def sigmoid --------------------------########\n",
    "##############################################################################\n",
    "def sigmoid(z):\n",
    "      result = 1 / (1+ np.exp(-z))    # 这地方也可以用 np.exp(-z) 一样的\n",
    "      return  result\n",
    "\n",
    "'''测试这个sigmoid 能否正常工作'''\n",
    "nums = np.arange(-10, 10, step=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,8))  # 这个命令，画图必用。。哈哈哈\n",
    "ax.plot(nums, sigmoid(nums), 'r')             # 就是 plot（x,y, 样式）\n",
    "plt.show()\n",
    "\n",
    "##############################################################################\n",
    "########---------------------sigmoid end--------------------------########\n",
    "##############################################################################\n",
    "\n",
    "'''3'''\n",
    "##############################################################################\n",
    "########---------------------cost function --------------------------########\n",
    "##############################################################################\n",
    "def costFunction(theta, X, y):   # costfunction 就是 J（theta）\n",
    "      theta = np.matrix(theta)\n",
    "      X = np.matrix(X)\n",
    "      y = np.matrix(y)\n",
    "\n",
    "      first_term = np.multiply(-y , np.log(sigmoid(X * theta.T)))\n",
    "      second_term = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n",
    "      jVal = np.sum(first_term - second_term) / (len(X))\n",
    "      return jVal\n",
    "'''下面是初始化， 初始化之后得到的结果是当theta = 【0，0，0】 时候的cost'''\n",
    "##  初始化加计算部分\n",
    "data.insert(0, 'ones', 1)\n",
    "cols = data.shape[1]\n",
    "features = cols-2\n",
    "\n",
    "X = data.iloc[:,0:(cols-1)]\n",
    "y = data.iloc[:,(cols-1):cols]\n",
    "\n",
    "X = np.array(X.values)\n",
    "y = np.array(y.values)\n",
    "theta = np.zeros(cols-1)\n",
    "costFunction(theta,X,y)\n",
    "##############################################################################\n",
    "########---------------------cost function end--------------------------########\n",
    "##############################################################################\n",
    "\n",
    "'''4'''\n",
    "'''这是我自己写的梯度下降，效果不是很好， 因为logistic regression 的回归 梯度以及学习率不好掌握'''\n",
    "##############################################################################\n",
    "########---------------------gradient descent--------------------------########\n",
    "##############################################################################\n",
    "x = X\n",
    "zero_vect = np.zeros(x.shape[1])\n",
    "theta = np.matrix(zero_vect)\n",
    "\n",
    "def gradient(theta,x,y,iter,alpha):\n",
    "      x = np.matrix(x)\n",
    "      y = np.matrix(y)\n",
    "      parameters = int(theta.ravel().shape[1])\n",
    "      p = parameters          # p 等于几 theta就有几个， 比如p=3， 就有 theta0， theta1，theta2\n",
    "      m = x.shape[0]          # m 是样本个数\n",
    "\n",
    "      temp = np.matrix(np.zeros(p))    # 初始化temp，让temp = 【0，0，0】， 最后的 temp 结果就是梯度下降后的 theta\n",
    "      cost = np.zeros(iter)\n",
    "\n",
    "      for i in range(iter):\n",
    "\n",
    "            z = (x*(theta.T))\n",
    "            error = sigmoid(z) - y\n",
    "\n",
    "            for j in range(p):\n",
    "\n",
    "                  term = np.multiply(error, x[:,j])\n",
    "                  temp[0,j] = theta[0,j] - ((alpha * np.sum(term)) / m)\n",
    "\n",
    "                  theta = temp\n",
    "                  cost[i] =  costFunction(theta,x, y)\n",
    "\n",
    "      return  theta, cost\n",
    "\n",
    "alpha = 0.02\n",
    "iter = 100\n",
    "final_theta, final_cost_matrix = gradient(theta,x,y,iter,alpha)\n",
    "final_cost = costFunction(final_theta,x,y)\n",
    "t = final_theta\n",
    "cm = final_cost_matrix\n",
    "c = final_cost\n",
    "print(t)\n",
    "print(c)\n",
    "\n",
    "## plot cost， 看看是不是从下降到converge\n",
    "fig, ax = plt.subplots(figsize = (12,8))\n",
    "ax.plot(np.arange(iter),final_cost_matrix,'r')   # np.arrange(iter) 是把1000重新arrange了， 变成了从 0 到 999\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error VS Training')\n",
    "plt.show()\n",
    "''''''\n",
    "##############################################################################\n",
    "########---------------------gradient--------------------------########\n",
    "##############################################################################\n",
    "\n",
    "'''5'''\n",
    "\n",
    "''' 用老师交的方法，  写function只包括求偏导的地方'''\n",
    "##############################################################################\n",
    "########---------------------def gradient--------------------------########\n",
    "##############################################################################\n",
    "\n",
    "zero_vect = np.zeros(X.shape[1])\n",
    "    # 初始化theta\n",
    "\n",
    "'''下面这个function 是梯度下降一步的计算，只下降了一次。'''\n",
    "def gradient(theta,X, y):\n",
    "\n",
    "      theta = np.matrix(theta)\n",
    "      X = np.matrix(X)\n",
    "      y = np.matrix(y)\n",
    "\n",
    "      parameters = int(theta.ravel().shape[1])\n",
    "      p = parameters\n",
    "      m = X.shape[0]\n",
    "\n",
    "      grad = np.zeros(p)            # grad 用来代替theta， 初始值是0\n",
    "      error = sigmoid(X*theta.T) - y\n",
    "\n",
    "      for j in range(p):\n",
    "            term = np.multiply(error, X[:,j])\n",
    "            grad[j] = np.sum(term) / m\n",
    "      new_theta = grad\n",
    "      return new_theta\n",
    "\n",
    "grad = gradient(theta,X, y)\n",
    "\n",
    "'''现在可以用SciPy's truncated newton（TNC）实现寻找最优参数'''\n",
    "import scipy.optimize as opt\n",
    "\n",
    "result = opt.fmin_tnc(func=costFunction, x0=theta, fprime=gradient, args=(X,y))\n",
    "   # 这一步命令，func 和 fprime 一定要变量顺序一致， costFunction(theta,x, y), gradient(theta,x,y) 也得是 theta，x，y这个顺序，\n",
    " ## 要不然这个命令没法跑！\n",
    "\n",
    "result      # 这个result是一个 元组\n",
    "costFunction(result[0],X,y)\n",
    "##############################################################################\n",
    "########---------------------def gradient--------------------------########\n",
    "##############################################################################\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# path ='/Users/jiayun/PycharmProjects/untitled/untitled/untitled/r' \\\n",
    "#       'bot Project/ my code/machine learning/week3 logistic regression/ex2data1.txt'\n",
    "#\n",
    "# data = pd.read_csv(path, header = None, names = [\"Exam 1\", \"Exam 2\", \"Admitted\"])\n",
    "# data.head()\n",
    "#\n",
    "# positive = data[data['Admitted'].isin([1])]\n",
    "# negative = data[data['Admitted'].isin([0])]\n",
    "#\n",
    "# fig, ax = plt.subplots(figsize=(12,8))\n",
    "# ax.scatter(positive['Exam 1'], positive['Exam 2'], s=50, c='b', marker='o', label='Admitted')\n",
    "# ax.scatter(negative['Exam 1'], negative['Exam 2'], s=50, c='r', marker='x', label='Not Admitted')\n",
    "# ax.legend()\n",
    "# ax.set_xlabel('Exam 1 Score')\n",
    "# ax.set_ylabel('Exam 2 Score')\n",
    "# plt.show()\n",
    "#\n",
    "#\n",
    "# def  sigmoid(z):\n",
    "#       return 1 / (1 + np.exp(-z))\n",
    "#\n",
    "# nums = np.arange(-10, 10, step=1)\n",
    "#\n",
    "# fig, ax = plt.subplots(figsize=(12,8))\n",
    "# ax.plot(nums, sigmoid(nums), 'r')\n",
    "# plt.show()\n",
    "#\n",
    "# def cost(theta, X, y):\n",
    "#       theta = np.matrix(theta)\n",
    "#       X = np.matrix(X)\n",
    "#       y = np.matrix(y)\n",
    "#       first = np.multiply(-y, np.log(sigmoid(X * theta.T)))\n",
    "#       second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))\n",
    "#       return np.sum(first - second) / (len(X))\n",
    "#\n",
    "#\n",
    "#\n",
    "# # add a ones column - this makes the matrix multiplication work out easier# add a\n",
    "# data.insert(0, 'Ones', 1)\n",
    "#\n",
    "# # set X (training data) and y (target variable)\n",
    "# cols = data.shape[1]\n",
    "# X = data.iloc[:,0:cols-1]\n",
    "# y = data.iloc[:,cols-1:cols]\n",
    "#\n",
    "# # convert to numpy arrays and initalize the parameter array theta\n",
    "# X = np.array(X.values)\n",
    "# y = np.array(y.values)\n",
    "# theta = np.zeros(3)\n",
    "#\n",
    "#\n",
    "# def gradient(theta, X, y):\n",
    "#       theta = np.matrix(theta)\n",
    "#       X = np.matrix(X)\n",
    "#       y = np.matrix(y)\n",
    "#\n",
    "#       parameters = int(theta.ravel().shape[1])\n",
    "#       grad = np.zeros(parameters)\n",
    "#\n",
    "#       error = sigmoid(X * theta.T) - y\n",
    "#\n",
    "#       for i in range(parameters):\n",
    "#             term = np.multiply(error, X[:,i])\n",
    "#             grad[i] = np.sum(term) / len(X)\n",
    "#\n",
    "#       return grad\n",
    "#\n",
    "# import scipy.optimize as opt\n",
    "# result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))\n",
    "# result\n",
    "#\n",
    "# cost(result[0], X, y)\n",
    "\n",
    "\n",
    "'''6'''\n",
    "'''prediction'''\n",
    "##############################################################################\n",
    "########---------------------def prediction --------------------------########\n",
    "##############################################################################\n",
    "def predict(theta, X):\n",
    "      # theta = np.matrix(theta)\n",
    "      # X = np.matrix(X)\n",
    "      probability = sigmoid(X * theta.T)\n",
    "      prediction = [ 1 if x >= 0.5 else 0 for x in probability ]   # prediction结果是一个list\n",
    "      return prediction\n",
    "\n",
    "'''上面的函数是求预测用的，下面求预测结果'''\n",
    "\n",
    "'''下面这一段代码是关于匹配正确率的，仔细学习一下'''\n",
    "theta_final = np.matrix(result[0])\n",
    "y_prediction = predict(theta_final,X)\n",
    "correct = [1 if((a==1 and b ==1) or (a==0 and b==0)) else 0 for (a,b) in zip(y_prediction, y) ]\n",
    "accuracy = (sum(map(int, correct)) % len(correct))\n",
    "print('accuracy = {0}%'.format(accuracy))\n",
    "##############################################################################\n",
    "########---------------------prediction end--------------------------########\n",
    "##############################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
