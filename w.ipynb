{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "path = '/Users/jiayun/PycharmProjects/untitled/untitled/u' \\\n",
    "       'ntitled/rbot Project/ my code/machine learning/week1 线性回归/ex1data2.txt'\n",
    "data = pd.read_csv(path, header = None, names = [\"size\", \"bedrooms\",\"price\"])\n",
    "data.head()\n",
    "data.describe()    # 统计描述\n",
    "\n",
    "# 特种标准化 这一步可以用dataframe的操作，很方便！！\n",
    "data = (data - data.mean())/(data.max() - data.min())   # 所有的这些data.mean(), data.max(), data.min() 都是对dataframe的 column 操作的\n",
    "\n",
    "\n",
    "data.insert(0, 'Ones', 1)  # 给初始数据插入一列1\n",
    "'''变量初始化 set X (trainning data) and y (target variance)'''\n",
    "cols = data.shape[1]   # 参数个数， 只有一个feature， 就是有两个参数， theta0 和 theta1\n",
    "'''找到data中x 和 y ， 其实一个是倒数第二列， 一个是最后一列'''\n",
    "X = data.iloc[:, 0: (cols-1)]\n",
    "y = data.iloc[:, (cols-1): cols]  # 其实也可以用 [:, -1]\n",
    "\n",
    "# ### 不同的行和列 调取方式\n",
    "# z = data.iloc[1:9, 0 : cols-1]\n",
    "# v = data.loc[:, 'population']\n",
    "# c = data.loc[5:15, 'population']\n",
    "# d = data.loc[5:15, :]\n",
    "X.head()\n",
    "y.head()\n",
    "\n",
    "'''numpy矩阵初始化'''\n",
    "X = np.matrix(X.values)\n",
    "y = np.matrix(y.values)\n",
    "theta = np.matrix(np.array([0, 0, 0]))\n",
    "#  调用代价函数计算\n",
    "\n",
    "\n",
    "# 创建 theta 函数\n",
    "##############################################################################\n",
    "########---------------------1.calculate cost---------------------------########\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "def computeCost(X, y, theta):\n",
    "    inner = np.power(((X * theta.T) - y), 2)\n",
    "\n",
    "    return np.sum(inner) / (2 * len(X))\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "########--------------------calculate cost end------------------------########\n",
    "##############################################################################\n",
    "\n",
    "computeCost(X, y, theta)\n",
    "\n",
    "\n",
    "\"\"\" batch gradient descent\"\"\"\n",
    "''' 这些是指导， 下面的是我自己写的\n",
    "def gradientDescent (X, y, theta, alpha, iters):\n",
    "    temp = np.matrix(np.zeros(theta.shape))\n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    cost = np.zeros(iters)\n",
    "\n",
    "    for i in range(iters):\n",
    "        error = (X * theta.T) - y\n",
    "\n",
    "        for j in range(parameters):\n",
    "            term = np.multiply(error, X[:, j])\n",
    "            temp[0, j] = theta[0, j] - ((alpha * len(X)) * np.sum(term))\n",
    "\n",
    "            theta = term\n",
    "            cost[i] = computeCost(X, y, theta)\n",
    "    return theta, cost\n",
    "    \n",
    "    '''\n",
    "\n",
    "################################################################################\n",
    "########---------------------2.gradient descent---------------------------########\n",
    "################################################################################\n",
    "zeros_vec = np.zeros(X.shape[1])   # 初始化 theta， [0,0]\n",
    "theta = np.matrix(zeros_vec)       # 把初始化的theta 变成 matrix\n",
    "def gradientDescent(X, y, alpha, theta, iters):\n",
    "    temp = np.matrix(np.zeros(theta.shape))  # 让temp = theta\n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    p = parameters\n",
    "    m = X.shape[0]\n",
    "    cost = np.zeros(iters)\n",
    "    for i in range(iters):\n",
    "\n",
    "        error = X * (theta.T) - y\n",
    "\n",
    "        for j in range(p):\n",
    "            term = np.multiply(error, X[:,j])\n",
    "            temp[0, j] = theta[0, j] - ((alpha * np.sum(term)) / m)\n",
    "\n",
    "            theta = temp\n",
    "            cost[i] = computeCost(X, y, theta)\n",
    "    return theta,cost\n",
    "\n",
    "################################################################################\n",
    "########--------------------gradient descent end------------------------########\n",
    "################################################################################\n",
    "\n",
    "# 初始化 并计算结果\n",
    "alpha = 0.01\n",
    "iter = 1000\n",
    "\n",
    "grad, cost = gradientDescent(X, y, alpha, theta, iter)# 调用上面的函数，求1000次下降后的theta， grad就是最终的theta\n",
    "\n",
    "final_cost = computeCost(X,y,grad)\n",
    "print(final_cost)\n",
    "print(grad)\n",
    "\n",
    "################################################################################\n",
    "########---------------------3.plot part---------------------------########\n",
    "################################################################################\n",
    "\n",
    "\n",
    "## plot cost， 看看是不是从下降到converge\n",
    "fig, ax = plt.subplots(figsize = (12,8))\n",
    "ax.plot(np.arange(iter),cost,'r')   # np.arrange(iter) 是把1000重新arrange了， 变成了从 0 到 999\n",
    "ax.set_xlabel('iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error VS Training')\n",
    "plt.show()\n",
    "################################################################################\n",
    "########--------------------plot part end------------------------########\n",
    "################################################################################\n",
    "\n",
    "\n",
    "################################################################################\n",
    "########--------------------4.compute normal equation------------------------########\n",
    "################################################################################\n",
    "def normalEquation(X,y):\n",
    "\n",
    "    theta_norm = np.linalg.inv(X.T@X) @X.T@y    # 符号@ 就是 * ，用*也一样\n",
    "                                                 # X.T@X 这命令也可以写成 X.T.dot(X)\n",
    "\n",
    "    return theta_norm\n",
    "################################################################################\n",
    "########--------------------normal equation end------------------------########\n",
    "################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
